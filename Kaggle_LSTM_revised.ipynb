{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os, os.path \n",
    "import pickle\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.optim import Rprop, Adam\n",
    "import math\n",
    "import time\n",
    "torch.set_default_dtype(torch.float)\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\"\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"\"\n",
    "#ROOT_PATH = \"./argo2/\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    #outputs = None\n",
    "    outputs = np.zeros((inputs.shape[0], 60, 2))\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "        \n",
    "          \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'palo-alto'\n",
    "#city = 'washington-dc' \n",
    "split = 'test'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split)\n",
    "inpu , out = train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402a0a2",
   "metadata": {},
   "source": [
    "batch_sz = 4 # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564fff0e",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "   \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1], alpha=0.5)\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1], alpha=0.2)\n",
    "\n",
    "        \n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      implement your Deep learning model\n",
    "      implement training routine\n",
    "    \"\"\"\n",
    "    \n",
    "    show_sample_batch(sample_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c98e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "# Calculate the distance between two points\n",
    "def calc_distance(a, b):\n",
    "    return math.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)\n",
    "\n",
    "# Calculate the velocity between two points\n",
    "def velocity(a,b):\n",
    "    return [(b[0] - a[0])/0.1, (b[1] - a[1]/0.1) ]\n",
    "\n",
    "def speed(a,b):\n",
    "    dist = math.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)\n",
    "    return dist/0.1\n",
    "# encoder city\n",
    "def onehot_city(city):\n",
    "    if city == \"austin\":\n",
    "        return [1,0,0,0,0,0]\n",
    "    elif city == \"miami\":\n",
    "        return [0,1,0,0,0,0]\n",
    "    elif city == \"pittsburgh\":\n",
    "        return [0,0,1,0,0,0]\n",
    "    elif city == \"dearborn\":\n",
    "        return [0,0,0,1,0,0]\n",
    "    elif city == \"washington-dc\":\n",
    "        return [0,0,0,0,1,0]\n",
    "    else:\n",
    "        return [0,0,0,0,0,1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e38642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tensor x\n",
    "def get_input_tensor(cities_dataset, split, val = False):\n",
    "    X = []\n",
    "    for city in cities_dataset:\n",
    "        #print(\"input x: \" + \"Processing city\", city)\n",
    "        dataset = ArgoverseDataset(city = city, split = split)\n",
    "        for i in range(cities_dataset[city]):\n",
    "            \n",
    "            if val is True:\n",
    "                i+= cities_dataset[city]\n",
    "            # city feature  \n",
    "            x1 = onehot_city(city)\n",
    "            X.extend(x1)\n",
    "            \n",
    "            # coordinate feature\n",
    "            inpu, _ = dataset[i]            \n",
    "            \n",
    "            # Record start/end position/velocity of the target car \n",
    "            end_pos_x = inpu[49][0]\n",
    "            end_pos_y = inpu[49][1]\n",
    "            \n",
    "            for j in range(inpu.shape[0]):\n",
    "                if j == 0:\n",
    "                    vel_x , vel_y = velocity(inpu[j], inpu[j])\n",
    "                    dist = calc_distance(inpu[j], inpu[j])\n",
    "                else:\n",
    "                    vel_x , vel_y = velocity(inpu[j-1], inpu[j])\n",
    "                    dist = calc_distance(inpu[j-1], inpu[j])\n",
    "                pos_x = inpu[j][0] -  end_pos_x\n",
    "                pos_y = inpu[j][1] -  end_pos_y\n",
    "                X.append(pos_x)\n",
    "                X.append(pos_y)\n",
    "#                 X.append(vel_x)\n",
    "#                 X.append(vel_y)\n",
    "#                 X.append(dist)\n",
    "\n",
    "            \n",
    "            feat_num_in =  len(x1)+ 100 #+ 100 + 50#len(x2)#+ len(x3) + len(x4) +  \n",
    "            \n",
    "    tensor_x = torch.tensor(X).reshape(sum(cities_dataset.values()), feat_num_in)  \n",
    "    return tensor_x\n",
    "\n",
    "#get tensor y \n",
    "def get_out_tensor(cities_dataset, split, val = False):\n",
    "    Y = []\n",
    "    feat_num_out = 120\n",
    "    for city in cities_dataset:\n",
    "        \n",
    "        dataset = ArgoverseDataset(city = city, split = split)\n",
    "        \n",
    "        for i in range(cities_dataset[city]):\n",
    "            if val is True:\n",
    "                i+= cities_dataset[city]\n",
    "            # coordinate feature\n",
    "            inpu, out = dataset[i]\n",
    "            end_pos_x = inpu[49][0]\n",
    "            end_pos_y = inpu[49][1]\n",
    "            \n",
    "            for j in range(out.shape[0]):\n",
    "                pos_x = out[j][0] -  end_pos_x\n",
    "                pos_y = out[j][1] -  end_pos_y\n",
    "                Y.append(pos_x)\n",
    "                Y.append(pos_y)\n",
    "               \n",
    "            \n",
    "            #y = list(out.reshape(120))\n",
    "            #Y.extend(y)\n",
    "    \n",
    "    tensor_y = torch.tensor(Y).reshape(sum(cities_dataset.values()), feat_num_out)\n",
    "    return tensor_y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c05567ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_train = {\"austin\":40000, \n",
    "               \"miami\":50000, \n",
    "                \"pittsburgh\":40000, \n",
    "                \"dearborn\":22000, \n",
    "                \"washington-dc\":23000, \n",
    "                \"palo-alto\":11000}\n",
    "# train dataset\n",
    "#cities_train = city_data('miami')\n",
    "train_tensor_x = get_input_tensor(cities_train, \"train\").to(device)\n",
    "train_tensor_y = get_out_tensor(cities_train, \"train\").to(device)\n",
    "# train_dataset = TensorDataset(train_tensor_x,train_tensor_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self,features, target, sequence_length=50):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y = target\n",
    "        self.X = features\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start:(i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0:(i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69254ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SequenceDataset(\n",
    "    train_tensor_x,\n",
    "    train_tensor_y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8786ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90c541a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_val = {\"austin\":3041, \n",
    "               \"miami\":5029, \n",
    "                \"pittsburgh\":3544, \n",
    "                \"dearborn\":2465, \n",
    "                \"washington-dc\":2744, \n",
    "                \"palo-alto\":993}\n",
    "\n",
    "# validation dataset \n",
    "val_tensor_x  = get_input_tensor(cities_val, \"train\", val = True).to(device)\n",
    "val_tensor_y = get_out_tensor(cities_val, \"train\", val = True).to(device)\n",
    "# val_dataset = TensorDataset(val_tensor_x, val_tensor_y)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d68de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SequenceDataset(\n",
    "    val_tensor_x,\n",
    "    val_tensor_y,\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3e051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56991a7a",
   "metadata": {},
   "source": [
    "### Build, Train model and Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.hidden_dim = 100\n",
    "#         self.layer_dim = 3\n",
    "#         self.input_dim = 156\n",
    "#         self.output_dim = 120\n",
    "\n",
    "#         # LSTM layers\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             self.input_dim, self.hidden_dim, self.layer_dim, batch_first=True, dropout=0.2\n",
    "#         )\n",
    "#         self.fc1 = nn.Linear(in_features = hidden_dim, out_features = int(hidden_dim / 2))\n",
    "#         self.act1 = nn.ReLU(inplace = True)\n",
    "#         self.bn1 = nn.BatchNorm1d(num_features = int(hidden_dim / 2))\n",
    "\n",
    "#         self.estimator = nn.Linear(in_features = int(hidden_dim / 2), out_features = 1)\n",
    "    \n",
    "\n",
    "#         # Fully connected layer\n",
    "#         self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda().requires_grad_()\n",
    "#         c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda().requires_grad_()\n",
    "#         out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "#         out = out[:, -1, :]\n",
    "\n",
    "#         # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "#         out = self.fc(out)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c82de211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = 256\n",
    "        self.layer_dim = 1\n",
    "        self.input_dim = 106#256\n",
    "        self.output_dim = 120\n",
    "        self.batch_size = 64\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_dim, self.hidden_dim, self.layer_dim, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features = self.hidden_dim, out_features = 4096)\n",
    "        #self.act = nn.ReLU(inplace = True)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(4096,2048)\n",
    "        self.estimator = nn.Linear(2048, 120)\n",
    "    \n",
    "\n",
    "    def init_hidden(self, size):\n",
    "        return torch.zeros(self.layer_dim, size, self.hidden_dim).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.shape[0], self.hidden_dim).requires_grad_().to(device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.shape[0], self.hidden_dim).requires_grad_().to(device)\n",
    "\n",
    "        # One time step\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        #size = x.shape[0]\n",
    "        #hidden_state = self.init_hidden(size)\n",
    "        #cell_state = hidden_state\n",
    "        \n",
    "        #out, _ = self.lstm(x, (hidden_state, cell_state))\n",
    "\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        out = self.act(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.estimator(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea584bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(model, x, y, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x.float())\n",
    "    loss = criterion(output, y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, output\n",
    "\n",
    "# get val loss function\n",
    "def getValLoss(model, criterion):\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    for i_batch, sample_batch in enumerate(val_dataloader):\n",
    "        \n",
    "        x_val, y_val = sample_batch[0], sample_batch[1]\n",
    "        output = model(x_val.float())\n",
    "        total_loss += criterion(output, y_val.float()).item()\n",
    "        batch_count +=1\n",
    "    return total_loss/batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ac26080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time0m 31s, epoch 0, Train loss: 35.07663953661058, Val loss: 24.151152928670246\n",
      "Time0m 57s, epoch 1, Train loss: 23.809872219806117, Val loss: 22.812458250257706\n",
      "Time1m 20s, epoch 2, Train loss: 23.340107010367976, Val loss: 25.086985694037544\n",
      "Time1m 45s, epoch 3, Train loss: 22.792145972158394, Val loss: 23.124792734781902\n",
      "Time2m 9s, epoch 4, Train loss: 22.408755151849043, Val loss: 22.46766471862793\n",
      "Time2m 34s, epoch 5, Train loss: 22.167241942238718, Val loss: 21.461434258355034\n",
      "Time2m 58s, epoch 6, Train loss: 21.938232596731694, Val loss: 20.966515858968098\n",
      "Time3m 22s, epoch 7, Train loss: 21.648068568121744, Val loss: 21.157162772284615\n",
      "Time3m 46s, epoch 8, Train loss: 21.447337010491538, Val loss: 21.436857011583115\n",
      "Time4m 11s, epoch 9, Train loss: 21.322854933067823, Val loss: 21.026206758287216\n",
      "Time4m 35s, epoch 10, Train loss: 21.126123915187755, Val loss: 20.905380884806316\n",
      "Time5m 0s, epoch 11, Train loss: 21.00708479236646, Val loss: 20.886794408162434\n",
      "Time5m 25s, epoch 12, Train loss: 20.79894516350766, Val loss: 20.697733137342667\n",
      "Time5m 48s, epoch 13, Train loss: 20.742479259712738, Val loss: 20.33831532796224\n",
      "Time6m 13s, epoch 14, Train loss: 20.595755337274564, Val loss: 20.3437688615587\n",
      "Time6m 37s, epoch 15, Train loss: 20.482346885098515, Val loss: 19.66023678249783\n",
      "Time7m 2s, epoch 16, Train loss: 20.415126990484627, Val loss: 19.87674882676866\n",
      "Time7m 26s, epoch 17, Train loss: 20.277484413188965, Val loss: 19.694873385959202\n",
      "Time7m 51s, epoch 18, Train loss: 20.24216959769266, Val loss: 20.00155512491862\n",
      "Time8m 14s, epoch 19, Train loss: 20.15234445762831, Val loss: 19.985187742445206\n",
      "Time8m 39s, epoch 20, Train loss: 20.063991192980733, Val loss: 20.302818722195095\n",
      "Time9m 3s, epoch 21, Train loss: 19.946211151868404, Val loss: 19.126855850219727\n",
      "Time9m 28s, epoch 22, Train loss: 19.893123800581495, Val loss: 19.582683987087673\n",
      "Time9m 52s, epoch 23, Train loss: 19.853271135645794, Val loss: 19.737223307291668\n",
      "Time10m 17s, epoch 24, Train loss: 19.772215834671087, Val loss: 19.79027345445421\n",
      "Time10m 40s, epoch 25, Train loss: 19.66970977120519, Val loss: 20.245127148098415\n",
      "Time11m 5s, epoch 26, Train loss: 19.62334361862062, Val loss: 19.012088987562393\n",
      "Time11m 30s, epoch 27, Train loss: 19.56792253868909, Val loss: 19.39218266805013\n",
      "Time11m 54s, epoch 28, Train loss: 19.52413858873638, Val loss: 18.85723198784722\n",
      "Time12m 19s, epoch 29, Train loss: 19.42569817633903, Val loss: 19.126881069607204\n",
      "Time12m 43s, epoch 30, Train loss: 19.372593881340013, Val loss: 19.647531509399414\n",
      "Time13m 8s, epoch 31, Train loss: 19.342805516978167, Val loss: 18.68443531460232\n",
      "Time13m 32s, epoch 32, Train loss: 19.26973714687425, Val loss: 19.524503495958115\n",
      "Time13m 57s, epoch 33, Train loss: 19.226791583593663, Val loss: 19.125189039442276\n",
      "Time14m 21s, epoch 34, Train loss: 19.155765417969675, Val loss: 18.802033530341255\n",
      "Time14m 45s, epoch 35, Train loss: 19.08923153094832, Val loss: 18.57118691338433\n",
      "Time15m 9s, epoch 36, Train loss: 19.052136938228287, Val loss: 19.08580674065484\n",
      "Time15m 34s, epoch 37, Train loss: 19.001681093255964, Val loss: 18.641903771294487\n",
      "Time15m 58s, epoch 38, Train loss: 18.95311525625013, Val loss: 18.376320521036785\n",
      "Time16m 22s, epoch 39, Train loss: 18.869237357330846, Val loss: 19.54898304409451\n",
      "Time16m 46s, epoch 40, Train loss: 18.83573619368809, Val loss: 18.062047322591145\n",
      "Time17m 11s, epoch 41, Train loss: 18.77079735583032, Val loss: 18.236110899183487\n",
      "Time17m 36s, epoch 42, Train loss: 18.717681735497717, Val loss: 19.089980867173935\n",
      "Time18m 0s, epoch 43, Train loss: 18.706966146227007, Val loss: 18.288685904608833\n",
      "Time18m 24s, epoch 44, Train loss: 18.651296087093765, Val loss: 18.10705608791775\n",
      "Time18m 48s, epoch 45, Train loss: 18.592840947438894, Val loss: 17.961763275994194\n",
      "Time19m 13s, epoch 46, Train loss: 18.53428551385523, Val loss: 17.868147956000435\n",
      "Time19m 36s, epoch 47, Train loss: 18.535444668243954, Val loss: 18.16650856865777\n",
      "Time20m 1s, epoch 48, Train loss: 18.488256230073816, Val loss: 18.48131709628635\n",
      "Time20m 25s, epoch 49, Train loss: 18.43067807531209, Val loss: 18.022375742594402\n",
      "Time20m 50s, epoch 50, Train loss: 18.389384460974117, Val loss: 18.037792841593426\n",
      "Time21m 13s, epoch 51, Train loss: 18.373259068027494, Val loss: 17.67957592010498\n",
      "Time21m 38s, epoch 52, Train loss: 18.319587120289732, Val loss: 17.5958186255561\n",
      "Time22m 2s, epoch 53, Train loss: 18.261320120861, Val loss: 18.087782859802246\n",
      "Time22m 27s, epoch 54, Train loss: 18.24832866361636, Val loss: 17.41816192203098\n",
      "Time22m 50s, epoch 55, Train loss: 18.199747841424617, Val loss: 17.780312008327908\n",
      "Time23m 15s, epoch 56, Train loss: 18.13487402827706, Val loss: 17.25091287824843\n",
      "Time23m 39s, epoch 57, Train loss: 18.114697738164555, Val loss: 17.631620830959744\n",
      "Time24m 2s, epoch 58, Train loss: 18.079924910512215, Val loss: 17.328821923997666\n",
      "Time24m 27s, epoch 59, Train loss: 17.9948243786471, Val loss: 17.581148359510635\n",
      "Time24m 50s, epoch 60, Train loss: 17.977599339178433, Val loss: 17.438091807895237\n",
      "Time25m 14s, epoch 61, Train loss: 17.968972553204622, Val loss: 17.47843254937066\n",
      "Time25m 38s, epoch 62, Train loss: 17.89319357972068, Val loss: 17.647605154249405\n",
      "Time26m 2s, epoch 63, Train loss: 17.89252385266425, Val loss: 17.450970225863987\n",
      "Time26m 25s, epoch 64, Train loss: 17.85393310519974, Val loss: 17.564920001559788\n",
      "Time26m 50s, epoch 65, Train loss: 17.812599220840145, Val loss: 17.603039423624676\n",
      "Time27m 13s, epoch 66, Train loss: 17.76179430958, Val loss: 17.479873339335125\n",
      "Time27m 38s, epoch 67, Train loss: 17.743578006690583, Val loss: 16.992948214213055\n",
      "Time28m 1s, epoch 68, Train loss: 17.671195558719223, Val loss: 17.33418761359321\n",
      "Time28m 25s, epoch 69, Train loss: 17.66138785001048, Val loss: 17.421472549438477\n",
      "Time28m 49s, epoch 70, Train loss: 17.641679964731516, Val loss: 17.10704231262207\n",
      "Time29m 12s, epoch 71, Train loss: 17.57977962379075, Val loss: 17.229899088541668\n",
      "Time29m 36s, epoch 72, Train loss: 17.530075486927537, Val loss: 16.98324966430664\n",
      "Time29m 59s, epoch 73, Train loss: 17.528578043394084, Val loss: 17.120873133341473\n",
      "Time30m 24s, epoch 74, Train loss: 17.41891656514087, Val loss: 17.005827373928494\n",
      "Time30m 47s, epoch 75, Train loss: 17.42529989494505, Val loss: 16.82983250088162\n",
      "Time31m 12s, epoch 76, Train loss: 17.366094471131316, Val loss: 17.02653037177192\n",
      "Time31m 35s, epoch 77, Train loss: 17.3282004878986, Val loss: 16.74562708536784\n",
      "Time31m 59s, epoch 78, Train loss: 17.315913753161777, Val loss: 17.423235681321884\n",
      "Time32m 23s, epoch 79, Train loss: 17.25789967770834, Val loss: 16.885331683688694\n",
      "Time32m 47s, epoch 80, Train loss: 17.26150239256185, Val loss: 16.94289747873942\n",
      "Time33m 10s, epoch 81, Train loss: 17.189144484562934, Val loss: 16.93765354156494\n",
      "Time33m 35s, epoch 82, Train loss: 17.160291624512098, Val loss: 16.44915305243598\n",
      "Time34m 0s, epoch 83, Train loss: 17.116800159615753, Val loss: 17.059217770894367\n",
      "Time34m 23s, epoch 84, Train loss: 17.122788280320407, Val loss: 16.68790488772922\n",
      "Time34m 48s, epoch 85, Train loss: 17.064213506864608, Val loss: 16.734914037916397\n",
      "Time35m 11s, epoch 86, Train loss: 17.04592303816379, Val loss: 16.501683447096084\n",
      "Time35m 36s, epoch 87, Train loss: 16.99977315608111, Val loss: 16.937584982977974\n",
      "Time35m 59s, epoch 88, Train loss: 16.988030682653342, Val loss: 16.52080726623535\n",
      "Time36m 23s, epoch 89, Train loss: 16.94208290144714, Val loss: 16.401863521999783\n",
      "Time36m 47s, epoch 90, Train loss: 16.899054742819015, Val loss: 17.412347687615288\n",
      "Time37m 11s, epoch 91, Train loss: 16.852633057487022, Val loss: 16.408280478583443\n",
      "Time37m 35s, epoch 92, Train loss: 16.828202832060907, Val loss: 16.46221531762017\n",
      "Time37m 59s, epoch 93, Train loss: 16.79016937568174, Val loss: 16.357491493225098\n",
      "Time38m 22s, epoch 94, Train loss: 16.760082846373514, Val loss: 16.135563850402832\n",
      "Time38m 47s, epoch 95, Train loss: 16.728937502697978, Val loss: 16.157907168070476\n",
      "Time39m 10s, epoch 96, Train loss: 16.694045041736803, Val loss: 16.261047575208877\n",
      "Time39m 34s, epoch 97, Train loss: 16.6504586355605, Val loss: 16.12361017862956\n",
      "Time39m 58s, epoch 98, Train loss: 16.624831339072067, Val loss: 16.28451093037923\n",
      "Time40m 22s, epoch 99, Train loss: 16.58225323582253, Val loss: 16.392034742567276\n",
      "Time40m 46s, epoch 100, Train loss: 16.545983746899008, Val loss: 16.613322999742294\n",
      "Time41m 10s, epoch 101, Train loss: 16.55535689226983, Val loss: 16.499579005771214\n",
      "Time41m 34s, epoch 102, Train loss: 16.493684385501112, Val loss: 16.513378461201984\n",
      "Time41m 57s, epoch 103, Train loss: 16.47198521037015, Val loss: 16.206728511386448\n",
      "Time42m 22s, epoch 104, Train loss: 16.4613290234944, Val loss: 16.292484601338703\n",
      "Time42m 45s, epoch 105, Train loss: 16.366041207420157, Val loss: 15.874901347690159\n",
      "Time43m 9s, epoch 106, Train loss: 16.386642594988665, Val loss: 15.888614230685764\n",
      "Time43m 32s, epoch 107, Train loss: 16.32652234309393, Val loss: 16.331578148735893\n",
      "Time43m 56s, epoch 108, Train loss: 16.298783819987925, Val loss: 16.159502877129448\n",
      "Time44m 20s, epoch 109, Train loss: 16.289400500614782, Val loss: 15.803056928846571\n",
      "Time44m 44s, epoch 110, Train loss: 16.244667923897453, Val loss: 15.85460811191135\n",
      "Time45m 8s, epoch 111, Train loss: 16.19928465342251, Val loss: 16.013238377041286\n",
      "Time45m 31s, epoch 112, Train loss: 16.186910129635038, Val loss: 15.76424471537272\n",
      "Time45m 55s, epoch 113, Train loss: 16.155513502126876, Val loss: 16.070953157212998\n",
      "Time46m 18s, epoch 114, Train loss: 16.09552588100322, Val loss: 15.683206452263725\n",
      "Time46m 43s, epoch 115, Train loss: 16.091196929707248, Val loss: 15.87172402275933\n",
      "Time47m 6s, epoch 116, Train loss: 16.019654080266594, Val loss: 15.738253593444824\n",
      "Time47m 31s, epoch 117, Train loss: 16.030592959239634, Val loss: 15.723785294426811\n",
      "Time47m 54s, epoch 118, Train loss: 15.993259229322131, Val loss: 15.397848553127712\n",
      "Time48m 18s, epoch 119, Train loss: 15.95087912815069, Val loss: 15.497702810499403\n",
      "Time48m 41s, epoch 120, Train loss: 15.93601853731862, Val loss: 15.446200794643826\n",
      "Time49m 6s, epoch 121, Train loss: 15.893900844047764, Val loss: 15.493948300679525\n",
      "Time49m 29s, epoch 122, Train loss: 15.86864730882858, Val loss: 15.295460912916395\n",
      "Time49m 53s, epoch 123, Train loss: 15.840193603306979, Val loss: 15.68427160051134\n",
      "Time50m 18s, epoch 124, Train loss: 15.770719661063085, Val loss: 15.483333269755045\n",
      "Time50m 41s, epoch 125, Train loss: 15.78546789219833, Val loss: 15.29848607381185\n",
      "Time51m 5s, epoch 126, Train loss: 15.730691260720677, Val loss: 15.204617818196615\n",
      "Time51m 29s, epoch 127, Train loss: 15.72015942106716, Val loss: 15.466935475667318\n",
      "Time51m 53s, epoch 128, Train loss: 15.660306770119997, Val loss: 15.724162313673231\n",
      "Time52m 16s, epoch 129, Train loss: 15.648044953774372, Val loss: 15.302716573079428\n",
      "Time52m 41s, epoch 130, Train loss: 15.619912446877946, Val loss: 15.221510569254557\n",
      "Time53m 4s, epoch 131, Train loss: 15.620370356865179, Val loss: 15.349735683865017\n",
      "Time53m 29s, epoch 132, Train loss: 15.571662053809282, Val loss: 15.231910175747341\n",
      "Time53m 53s, epoch 133, Train loss: 15.531591396541746, Val loss: 15.132090250651041\n",
      "Time54m 17s, epoch 134, Train loss: 15.462646225862663, Val loss: 15.16777918073866\n",
      "Time54m 41s, epoch 135, Train loss: 15.464276741573727, Val loss: 14.85456297132704\n",
      "Time55m 5s, epoch 136, Train loss: 15.386699613525895, Val loss: 15.033269776238335\n",
      "Time55m 29s, epoch 137, Train loss: 15.371446021447118, Val loss: 15.099872589111328\n",
      "Time55m 53s, epoch 138, Train loss: 15.342296222251102, Val loss: 14.867655330234104\n",
      "Time56m 17s, epoch 139, Train loss: 15.29911447416108, Val loss: 14.71305709415012\n",
      "Time56m 41s, epoch 140, Train loss: 15.296944497498524, Val loss: 14.76836151546902\n",
      "Time57m 5s, epoch 141, Train loss: 15.253850750222746, Val loss: 14.769682990180122\n",
      "Time57m 28s, epoch 142, Train loss: 15.20904389270852, Val loss: 14.784310446845161\n",
      "Time57m 53s, epoch 143, Train loss: 15.166822983138456, Val loss: 14.939526557922363\n",
      "Time58m 16s, epoch 144, Train loss: 15.14360069203385, Val loss: 14.80063427819146\n",
      "Time58m 41s, epoch 145, Train loss: 15.138300846645748, Val loss: 14.780437363518608\n",
      "Time59m 4s, epoch 146, Train loss: 15.070064704607638, Val loss: 14.768293380737305\n",
      "Time59m 28s, epoch 147, Train loss: 15.043882577329883, Val loss: 14.780227025349935\n",
      "Time59m 52s, epoch 148, Train loss: 15.019803269605765, Val loss: 14.630482143825954\n",
      "Time60m 16s, epoch 149, Train loss: 14.996629702827551, Val loss: 14.822493765089247\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optm = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optm, milestones = [10,20,40], gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optm, mode='min', factor=0.1, patience=2, eps=1e-10, verbose=True)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "\n",
    "EPOCH_LENGTH = 150\n",
    "BATCH_SIZE = 64\n",
    "patience = 2\n",
    "trigger_times = 0\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(EPOCH_LENGTH):\n",
    "    total_loss  = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    my_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True) \n",
    "    for i_batch, (x_train, y_train) in enumerate(my_dataloader):\n",
    "        loss, _ = train(model, x_train, y_train, optm, criterion)\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    \n",
    "    #calculate train loss and val loss\n",
    "    train_loss = total_loss/batch_count\n",
    "    val_loss = getValLoss(model, criterion)\n",
    "    all_train_loss.append(train_loss)\n",
    "    all_val_loss.append(val_loss)\n",
    "    \n",
    "    # log\n",
    "#     if (epoch+1) % 10 == 0:\n",
    "    print('Time{}, epoch {}, Train loss: {}, Val loss: {}'.format(timeSince(start), epoch, train_loss, val_loss))\n",
    "   \n",
    "    scheduler.step(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9aefa90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f60760cd8b0>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47935331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5e4c6a5fd0>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyDElEQVR4nO3deXzU1b3/8deZmcxMJstk3xMCCYR9MwKCCgruCtpar1qXqq2113ur/XXXLtfbq+2t1mtba5UqbrV2c1+ouCAIyBIIq2FJWLKSfd8mM3N+f8xkSICQAEkmk/k8Hw8eTL7f78x8Bsibk/M9i9JaI4QQIvAY/F2AEEKIMyMBLoQQAUoCXAghApQEuBBCBCgJcCGECFCm4XyzuLg4nZmZOZxvKYQQAW/r1q01Wuv4448Pa4BnZmaSl5c3nG8phBABTyl15GTHpQtFCCEClAS4EEIEKAlwIYQIUBLgQggRoCTAhRAiQEmACyFEgJIAF0KIABWQAV7Z1MGqPUf9XYYQQvhVQAb4XzYVc8+ft+Jwuv1dihBC+E1ABnhzhxO3htZOp79LEUIIvwnIAG9zeIK7RQJcCBHEAjLAu4O7uUMCXAgRvAIywNscLkBa4EKI4BaQAd7d993S2eXnSoQQwn8CMsC7W+DShSKECGYBGeCtchNTCCECM8DbOr194NICF0IEsYAM8FYZhSKEEIEX4Fpr6UIRQggCMMA7nW7c2vNYWuBCiGAWcAHec/q8DCMUQgSzgAvw7iGEIF0oQojgFnAB3t3/DTIKRQgR3AIvwL2t7giriWZpgQshglgABrinCyUx0iotcCFEUAuIAH/ykwPcuPxz4NhSsomRFukDF0IEtX4DXCmVrpRarZQqUErtUUrdd9z57ymltFIqbqiKbOpwkl/c4BkD3t0Cj7DS5nDh6h5TKIQQQWYgLXAn8F2t9SRgHnCvUmoyeMIduAQoHroSISbMTKfTTZvDdawFbrcCMhJFCBG8+g1wrXWF1nqb93EzUACkek//H/ADYEibwTE2MwB1rQ5avC3wpEhPgDd3yFhwIURwOq0+cKVUJjAL2KSUWgqUaa139POcu5VSeUqpvOrq6jMqMibsWIC3OZwoBXHhFkBa4EKI4DXgAFdKhQOvAffj6VZ5EPhZf8/TWi/XWudqrXPj4+PPqMiY8GMB3trpIsxsIsJqAmQsuBAieA0owJVSIXjC+xWt9etAFjAW2KGUOgykAduUUklDUWTscS1wm9lIuDfAZSy4ECJYmfq7QCmlgOeAAq314wBa611AQo9rDgO5WuuaoSgyukeAtzpchFlMRFikBS6ECG4DaYEvAG4FLlZKbff+unKI6+olwmIixKiobXXQ1tm7BS594EKIYNVvC1xrvQ5Q/VyTOVgFnYxSipgwM/WtDlo6nYRZTIRLC1wIEeQCYiYmQLTN7GmBO1yEmY2EmaUPXAgR3AImwGPDzdS1dtLqcGKzmDAYFOEWk7TAhRBBK2ACPCbMQn1bF22dnhY44Alw2dRBCBGk+u0DHylibCHUtnSiAZu3+yTCapKbmEKIoBU4AR5moanDiUFBmMXbAreaZF9MIUTQCpwuFO9sTLc+1gIPt0iACyGCV+AEuHdBK8A3hFC6UIQQwSxwAjzsWIDbet7ElBa4ECJIBUyAx4YfC/AwS3cXSoi0wIUQQStgAjzadpIWuLcLxS278gghglAABXiI73F3Czwq1HOsttXhl5qEEMKfAibATUYDUd4Q726BZyWEA1BU3eK3uoQQwl8CJsDh2I3M7lEo470BfqBKAlwIEXwCK8C9/eDd48CT7VbCLSYKK5v9WZYQQvhFYAW4twXePRNTKUV2Qri0wIUQQSmgAjw23IxSYDUZfcfGS4ALIYJUQAX4OWNimJMZg8FwbH+J8YnhVDd30tAmI1GEEMEloAL8+nPS+Ns3z+t1bHxCBACF0goXQgSZgArwk8mWkShCiCAV8AGeGhVKaIiR/TISRQgRZAI+wA0Gz0gU6UIRQgSbgA9w8I5EqZQAF0IEl1ER4NmJ4Rxt6qC5Q/bHFEIEj1ER4OnRNgDKGzr8XIkQQgyfURHgSXYrAEebJMCFEMFjdAR4pDfAG9v9XIkQQgyfURHgib4A7/RzJUIIMXxGRYCbTQbiws0cbZIWuBAiePQb4EqpdKXUaqVUgVJqj1LqPu/xR5VSe5VSO5VSbyilooa82lNIjLRytFH6wIUQwWMgLXAn8F2t9SRgHnCvUmoy8CEwVWs9HdgP/Hjoyuxfst1KhQS4ECKI9BvgWusKrfU27+NmoABI1Vqv0lp3bwm/EUgbujL7l2S3yigUIURQOa0+cKVUJjAL2HTcqTuBlYNU0xlJirTS0NZFR5fLn2UIIcSwGXCAK6XCgdeA+7XWTT2OP4inm+WVPp53t1IqTymVV11dfbb19inJHgog/eBCiKAxoABXSoXgCe9XtNav9zh+O3A18FWttT7Zc7XWy7XWuVrr3Pj4+MGo+aR8Y8GlG0UIESRM/V2glFLAc0CB1vrxHscvB34ILNRatw1diQPjm40pLXAhRJDoN8CBBcCtwC6l1HbvsQeA3wEW4ENPxrNRa33PUBQ5EDKdXggRbPoNcK31OkCd5NT7g1/OmQu3mIiwmKQFLoQIGqNiJma3RLtM5hFCBI9RFeDJdisV0oUihAgSoyrAEyOtVEoLXAgRJEZVgCfbrVQ1d+B0uf1dihBCDLlRFeBJdituDVXNvZeVdTjd/GVTMQ6nBLsQYvQYVQGekxgBwJ7ypl7H/7m1lAfe2MXK3RX+KEsIIYbEqArwqal2TAZFfnF9r+N/zysBYH1hjT/KEkKIITGqAtwaYmRSciT5xQ2+Y/srm9le0oDZZGB9YS19zPgXQoiAM6oCHGBWRhQ7ShtwuT1B/bctJYQYFf++KIuyhnaO1Pp91r8QQgyKURngbQ4X+yubcTjdvJFfxpJJiSydkQLA+iLpRhFCjA6jL8DTowHYXtLAm/ll1LU6uOHcdMbGhZFst7KhsNbPFQohxOAYyGJWAWVMrI1oWwjrCmvYfKiOWRlRLJoQj1KKBdlxfFxQidutMRhOtryLEEIEjlHXAldKMSsjmvd2VlDd3MlPr56Md7VEFmTHUt/WxdbjRqkIIUQgGnUBDjArPQqApTNSmJ0R7Tt+UU4CCREWvv1qPuUN7X6qTgghBseoDPBLpyQxKyOKH14xsdfxKJuZF+6YQ0uHk689v5l/7T5KSZ2MShFCBCY1nOOic3NzdV5e3rC9X182FNVw1wt5tHs3QP7Vl6Zx45wMP1clhBAnp5TaqrXOPf74qGyB92d+Vhxbf7qEt+5dwJzMGH71r700tnX5uywhhDgtQRngADaziRnpUTy0bApN7V389uMD/i5JCCFOS9AGeLdJyZHcOCeDlz4/TFF1i7/LEUKIAQv6AAf4f5dMwKAUf91c7O9ShBBiwCTAgbhwC/OyYvm4oMrfpQghxIBJgHstmZTAwZpWDko3ihAiQEiAe108MQFAWuFCiIAhAe6VFm1jYlIEHxVU+rsUIYQYEAnwHhZPSiDvSL2MCRdCBAQJ8B4WT0rE5dZ8+ekNXP7EWj7ZO/DWuOz0I4QYbhLgPcxMi+JLs1JJtls5UtvGyl1HB/zc+/+2nf98NX8IqxNCiN4kwHswGBSP/9tMXr5rLjPS7RQOcERKfauD93ZWsKessd9r38gv5ct/3CAtdiHEWZMA70NWfDhFVS0DCtoP9hzF6dZUN3f2e+1b28vZeqSeVodrMMoUQgSxfgNcKZWulFqtlCpQSu1RSt3nPR6jlPpQKXXA+3t0f68VSLITwmnqcFLd0n8ov7uzAoDmTicdXX0Hs8ut2XrEs5lEzQDCXgghTmUgLXAn8F2t9SRgHnCvUmoy8CPgY631eOBj79ejRlZ8OABFVa2nvK6mpZMNRTWk2K0Ap2yF7zvaTHOHE4DaVglwIcTZ6TfAtdYVWutt3sfNQAGQCiwDXvRe9iJw7RDV6BfZCd4A76MffH9lMx8XVPLKxmLcGm49LxPwBHpfthyu8z2ubnYMXrFCiKB0WpsaK6UygVnAJiBRa10BnpBXSiX08Zy7gbsBMjICZ9OEZLsVm9lIYdWJAe5ya257bjNHmzoAT9gvyI4FTt0C33y4DpvZSJvDJS1wIcRZG3CAK6XCgdeA+7XWTd0bBfdHa70cWA6eHXnOpEh/UEp5bmR6W+Baa9/myJsO1XK0qYPvX5ZDYqSVycmRRIeFAFDTcvKWtdaaLYfqWJQTz/u7jlIjLXAhxFka0CgUpVQInvB+RWv9uvdwpVIq2Xs+GRh1i4hkxYdRVNWC0+Xmhmc+5zer9gHwVn45YWYjdy4Yy/XnpDE5JZLYMAvQdwu8pK6dquZOzsuKwx4aIi1wIcRZG8goFAU8BxRorR/vcept4Hbv49uBtwa/PP/KTginvLGDVzcXs+VwPX9YXUh+cT3v767gsqlJhJqNvmvNJgNRtpA++8A3FNUAMCczhthwM7V9tNSFEGKgBtKFsgC4FdillNruPfYA8Cvg70qpu4Bi4CtDUqEfdY9E+dXKveQkRlDT0smdL2yhucPJspmpJ1wfH245oQVe3tDOYx/s483tZaRFhzI+IZy4cMuAhicKIcSp9BvgWut1QF8d3osHt5yRpXskSqvDxf+7dAKNbV384LWdxIaZWZAVe8L1ceGWXi3wji4XX3t+M8V1bdx1/li+ceE4DAZFXLiZfUebh+1zCCFGp9MahRJsxsSGYTQochIjuHRyIlrDRwWVzMyIwmQ8sfcpPsLCjtIG39ePfrCP/ZUtPH/HuVyUc2yQTly4hfUttad877pWB09+UsgPLs/BGmI85bVCiOAkAX4KZpOBX31pGlNS7CilUAqW35bb5/Vx4RbfDMsNhTU8t+4Qt84b0yu8AWLDLDS2d+FwujGbTn4b4r1dFaxYf4glkxOYnxU3eB9KCDFqyFoo/fhKbjqTUyIHdG18hIVWh4s2h5Plnx0kNSqUB66cdMJ1cRFmwNPK7svOkgYAjjZ6xpprrdl8qO6MFsF6+L0v+PAL2ahCiNFGAnwQxYV7grmyqZOth+tZmBPfa6RKt+4hh6eatbmz1LOyYYU3wLeXNHDDM5+zcvfAl7gFT/C/uOEI/zrN5wkhRj4J8EEUH+EJ5nUHqmnudDInM+ak13UHfV8B3uZwcqDKc5OzvKEdgIPVnjVZ1uyrPq2amtqdOFxuGtpk2KIQo40E+CCKC/cE+PvejSDOHdtXgHuuq21xsPVIHXe+sAWH0+07v7usCbe3p6S7C6Wkvg2AdYU1p9WNUt3ieX5Du2wTJ8RoIwE+iBK8LfBNh2pJsVtJjQo96XWx3hZ4bWsnr2wq5pO9Vew92uQ7v9M7kmVGmp3y7gCv87TEyxraOVRz6hUSe+peNKteWuBCjDoS4IMoJsyMUuDWfbe+AcItJiwmA9XNnazd7+kS6e7zBthR2khqVChTU+1UNHqCu6S+jWTvkrWfHagZcE3dE4YaZKNmIUYdCfBBZDIaiLF5Wtfn9tH/DZ6FsuLCLazZX+1b/GpXjwDfWdrA9DQ7KVGhNLR10e5wUVrXxnnjYsmIsZ1WgHcPa2xoc+B2B8xaYkKIAZAAH2Td/dtzTtECB083yv5Kz0qHU1Mj2endT7OhzcGR2jamp0X5WtzFdW1UNHWQFmPj/PFxbDxYS5fL3edr99TdAndrz45BQojRQwJ8kMVHWLCHhpDtXUelL91BPz3NzqIJCRyobKajy8V27/jvGWl2krwBvvVIPVpDenQoF2TH0dLpJL+4YUD19FybpfG4bhSH0z3g/wiEECOPBPggu+uCsfzs6skYDKdeLz02zNPVsmhCPNPS7DjdmoKKJt7aXk6ExcSsjGhS7J6boJsPeabdp0XbWDA+DrPRwAd7eo/rbne4mPvIR7yZX9breM+hisffyLznz1v54Ws7z+yDCiH8TgJ8kF2Uk8CXz0nr97o474iVhTkJTEu1A7B2fw3v7argutmphJqNvhb4lsOejZDTY0KJtIZw4YQ4Vu6q6NWnvae8kcqmzhOCvbq5E3uoZ7OJ4wN8Z2kDO7wtfiFE4JEA95NFE+K5anoyM9LsJNutxIWbeWZtEQ6nm5vneraes4YYiQ0zU9bQjsmgSPa2yK+clkx5Ywf5PcK3u+vl+On2NS2djPeuqtjYYyx4u8NFTYuDkvp2ubkpRICSAPeTueNi+cPNszEZDSilmJZqp83hYlZGFBOTjq290t0KT4kKxejtllkyORGz0cD7uyp81+3wjmKpbXX4toFzuzU1LQ7GJ3oCvL7H2itlDZ6JQQ6nm8rmjiH8pEKIoSIBPkJ0d6PcPKf3xs/dre70mGOTgjzdKPG836MbZUdJA5OSPcG/6VAd4Jl96XJr38YUPWdjdk8MAiiubRvsjyOEGAYS4CPE0pkpfGlWKldPT+l1PCXK0wJPj7b1On7V9CQqGjvIL6mnvtVBcV0by2amkBBhYdNBT4B3j0BJsluJtJp6TeYprT8W2kfqJMCFCEQS4CNEdkIEj//bzBNWLzzWAu8d4EsmJRJuMfHChiO+TSRmpEUxZ2yMrx+8ewRKXLiFKJu5103Mkvp2zCYDBgUlEuBCBCTZ0GGE657Mkxbde12VCGsIX52XwZ/WHiTE6NlsYlqanblVMby7s4KSunZfCzw+wkK0LeSEFnhadCgOp5sj0oUiRECSFvgINz3NTly4mZnpUSecu2vBWEwGA69vKyM7Ppxwi4m54zx7dW48WOtrgcdHWLDbzL2WlC2tbyct2saYWBvF0gIXIiBJgI9w4+LDyfvJJYyJDTvhXEKk1TfmfIY34LPjw0mxW3lnZznVzZ2YTQYiLCaibSHUt/W8idlGenQoGTES4EIEKgnwAPfNC8dhMRk4z9vyNhgUN87J4LMDNWw5XEd8uAWlFNE9WuAtnU7q27pIi7aRERNGXauD5g5ZrVCIQCMBHuAy48LY9MBirpuV6jt247npGA2KbcUNvhmf9tAQmjqcOF1u3wiUtOhQxsR6bo4OpBW+u6yR59YdGoJPIYQ4ExLgo0CUzdxr7ZWESCuXTk4EIN67aFa0zTOdvqnDSal3DHh6jI0M7+iWgYxEeWzVPn7x7hfUnmIvTyHE8JEAH6W+OncMAPERnkWzorzrlNe3OXzbs6VFh5LhbYH3NxKlpqXTtw553pH6IalZCHF6JMBHqflZsVwzI4WLchIAiPK2wBvaHJTWtxPqXWcl0hpClC2k3y6U93dV4HJrDAryDtcNef1CiP7JOPBRymBQ/P6mWb6vo70t8Ia2Lt8YcKU83S5jYmzsKW9Ca+07dry3tpczMSmCSGuIb3VEIYR/SQs8SHS3wGtbHOw92txrYtC1s1LZXtLAB3sqT/rckro2th6pZ+nMFHIzo9ld1kibQ3b3EcLfJMCDRHcf+DNrizhS29ZrzfJb540hJzGCX7z7Be0O1wnPfXtHOQBLZ6RwbmYMTrf2LV8rhPCffgNcKbVCKVWllNrd49hMpdRGpdR2pVSeUmrO0JYpzlaExYRBQVF1KxdOiOeqacm+cyajgYeWTaGsoZ2nPi3s9TytNW/ml3FuZjRp0TZmj4lGKcjrpxulRkaqCDHkBtICfwG4/LhjvwYe0lrPBH7m/VqMYAaDIspmxmIy8D/Lpp7Q1z1vXCzXzUrlqU+Let2k3Hu0mQNVLSyd6Rlnbg8NIScxgk2Hajlc08qhmtYT3mvN/mrmPPzRSc8JIQZPvwGutV4LHD/sQAPduw7YgfJBrksMgZvmpPOLa6f6hg4e76FlU0iNCuXbr+b7Nn94a3s5JoPq1WI/NzOG9YW1LHrsUy79vzVUNfXeEGLdgWrcGnaVNQ7dhxFCnHEf+P3Ao0qpEuAx4Md9XaiUutvbzZJXXV19hm8nBsP3L5vIDbnpfZ6PtIbw5M2zqG7p5Nt/zae108k7O8q5YHwcMd5NmAG+ccE47l8ynh9ePpEul+aTvVW9Xie/uAGAwqqWIfkcQgiPMw3wbwHf0VqnA98BnuvrQq31cq11rtY6Nz4+/gzfTgyX6WlRPHztNNYX1nD5b9dS1tDOtT2m6QNkxNq4f8kE7lk4jtSoUD4qOBbgDqfb1/IukgAXYkidaYDfDrzuffwPQG5ijiI3nJvOM7fmUtPsIDTEyJJJiSe9TinF4kkJrCuspqPLM3pl79EmOp1uLCYDB6qah7NsIYLOmQZ4ObDQ+/hi4MDglCNGiksmJ/LOfy7gpbvmEGbpe77X4kmJdHS52VDkmWa/zTvN/vKpSRyqacXpcqO15rMD1b79O4UQg2MgwwhfBT4HcpRSpUqpu4BvAL9RSu0AHgHuHtoyhT9kJ0RwbmbMKa+ZNy6GMLPR142SX9JAYqSFC8bH0+XSFNe18XFBFbc+t5nV+6pO+VpCiNPT71R6rfVNfZw6Z5BrEQHIYjJy4YR4Pi6opGvpFPKLG5idEU12QjjguZHZfZNze0kDi/vojhFCnD6ZiSnO2rKZqVQ2dXL9059TXNfGrIwoX4AfqGrhY2+A7yj13Nx0uTV/WnvQN1SxL10uN/f+ZRtv5Jee9LzW0iUjgpsEuDhrl09N4rc3zmT/Uc9Ny1kZ0YRbTCTbrbyZX0Z1cydRthB2lTagtWZDUQ0Pv1/AivWezSHcbs2znx3kYHXvUSvL1x7kvZ0V/OytPb6w31/ZzH+9vYfc//mIO17YQpfLPbwfVogRRAJcDIplM1N56z8W8KMrJjI7IxqA7IRwDlS1YFBw54Kx1Ld1UVrfzqf7PPMB3tpejtaaNQeq+Z/3CrjrxTzf1m6Halr57ccHyB0TTWunk99/UsiWw3UsfXIdf9lcTE5SOJ/uq+ZHr+2SlrgIWhLgYtBMSIzgnoVZGL27A2XFe7pRZmdEs3CCZw7ArrJG1uyvxhpioLiujW3FDbyw/jCRVhNHalv58eu7+Lyolvv/mo/FZOCpr87m385N5+WNh7nz+S2k2ENZ94OLeOXr87h/yXhe21bK02sO+u0zC+FPEuBiyIxP9AT4xZMSmJgcQYhRsXL3UQqrWvjWwmwsJgNPfLSfNfuruev8cXzvshze3VnBTX/aSFF1K7/80jQSIq18Z8kEQowGIkND+PPX55IQaQXgvsXjWZAdy9+2FJ/w3p/uq+KdHWe/wkObw8ljH+wb0uVzC6taKJQx8+IMyIYOYsjMGxdLZqyNq6elYDEZmZgUybs7PaF61fRkDlQ18+7OCkKMipvnZhAbZsagFMl2K5dOTiLUbAQ8e3y+/R/nE20LIda7xyd4JhKdnx3P//5rL7UtncSGW3C7NU98tJ/ffVKI2WRgYU48kdaQM/4Ma/ZV8+TqQnKSIrhmRsrZ/YH04Sdv7qLLpXntW/OH5PXF6CUtcDFksuLD+fT7F/kWz5qWZkdrz16cWfFhXOtd4fDq6SnER1gwGBT3LMxi2cxUX3h3y04I7xXe3WZnRAH41if/5coCfvdJIReMj8PhdLOqj00qBqp7PZfdQ7gwV2l9O2X17UP2+mL0kgAXw2ZGmh2ARTnxKKVYmBPPNxeO4ztLJpzxa05Pi8JkUGwrrsfpcvOPraVcOS2Jl+6cQ2pU6Fl3oxR6R8bsLB2aAHe7NVVNnVQ1d+CUETXiNEmAi2EzZ2wsIUbFlVM9S9OGGA38+IpJfS5vOxChZiOTkiPZdqSBzYfqaGjrYumMFJRSXDMjhXWFNdS1Onjsg33c8PTnpx2SRd4A313eOCRLAdS1OXC43Lg11LScely8EMeTABfDZmxcGDt/fhnzs+MG9XVnZ0Sxo7SB93dXYDEZuNA74uWaGcm43Jp7Xt7Kk6sL2Xy4rs99P0/G7dYUVbUSZQuhucPJkbq2Qa0b4GjjsbXUKxqlG0WcHglwMayO79seDLMyomlzuPhHXikXTojHZvbcm5+cHMm4+DA2H67j8ilJjIm18dw6z5DDf24t5UtPraesoe/QLG9sp73LxdXTPT8xDMUGFT0DvPK4jTGE6I8EuAh43ROHOp1uLpuS5DuulOL+JRO4dmYKT9w4k6/Nz2RbcQPPrz/Ej1/fybbiBm59dhPVzb3372zp9AwZ7L6BecXUZMwmwylvZLrdmp++uZudpQ2nVfvRpp4tcAlwcXokwEXAS48JJS7cjNGgWDIpode5pTNSeOLGWVhDjHwlN50Ii4mH3vmCJLuV527PpaKxg9tWbPatZ/5Gfimz/nsVByqbfQE+MSmCScmRpwzn4ro2Xt54hOfXHz6t2iubOjAoMBsNvcJciIGQABcBTynPnp1XT08mymbu87pwi4nb5o8hNMTI07ecw+JJifzhq7MoqGjiqdWFtDtc/O/KfXS5NH/dUkJRtaf/OybMzPRUO3vKmihraOeJj/bz+Kp9PPvZQZq8U//3Hm0CPBOIXH3c7Nx6pI4jtb03eq5o7CA+wkKS3dqrO0WIgZCJPGJUeGjZ1AFd971Lc7j7wizsoZ7JPRdPTOTamSk8veYgR5s6ONrUwfiEcN7ILyMjxkZ2fDhKKaal2nl54xEWPboap1vTvfyKUoq7zh/LXu9CXvVtXewobfB163RzuTV3vpDH7Iwonr/j2AZWlU0dJNlDsRgNEuDitEkLXAQVpZQvvLs9eNVkLCEG/p5XyiWTE3ngqknUtTrYXtLgWxb3vKxY7KEhXDMjhbXfv4iDj1xJalQo24o9OxDtO9rsmYykYPXeEzeu2Hu0icb2LjYfquu1guLRxg6SIr0tcOlCEadJAlwEvfgICz+9ajLRthB+ePlELhwfT5J3vZXuAE+PsbHj55fy+A0zSY+xYTAoZmVEkX/kWICfkxHNOWOiT7rz0MaDdQC0Oly9+tKPNnaQbA8lyW6lorFDVlYUp0UCXAg8GzlveXAJ2QnhGA2K689JAyDLG+AnMysjmvLGDg7XtHKotpWcpAgW5SSwu6yJvMN1PP7hfl9YbzpYS5x3KYANhbUAtHY6ae50khhpJSnSisPppqGta2g/qBhVJMCF8DIZj307fG1BJl+bn8ncsX3vCdq9Dsvf80rQ2jNa5eKJnlEw1z/9Ob/7+AAPvrEbt1uz+XAdF0+MZ3JyJOu9G0B3d5kk260k2T0tfhlKKE6H3MQU4iTiwi3819Ipp7xmSoods8nTdw6QkxTB2Lgwbp6bQYzNjDXEwGOr9vP8hsM0tHUxd6ynH/3FDUfo6HL5blomRlqxhHj+86hs6mBySuRZ1a61Zk95E1NSIlFKndVriZFNWuBCnCGzycDUlEhqWjqxhhgYExuGUopHrpvG9y7L4esXjCMu3ML/rtwLwNxxMczPisPhcpN3uN4X4El2q6/PfTBa4C9uOMzVv1/HL1fulT71UU5a4EKchdkZ0WwrbmBCYoRvJ6Ju1hAjdyzI5NEP9pEWHUpatI0omxmTQfHZgWoivaNhkiKtmIwKg+KsR6J0dLn445oiwsxGlq89iMmgmJkeRWN7F9fOSiXEKG220UQCXIizMCsjGjhETmLESc/fMm8MT39axPneBbzCLSYW5cTz/PrDTEqOwB4a4lsfJi7cwtHGdjYerKXL5eaC8fGnXc8/tpZS2dTJy3fN4c38cp76tMh3TgM35Kaf9muKkUsCXIizkJsZjcmgmJ4eddLz9tAQ3r/vAuy2Y2PPf/OVmXzlmQ3sKG1kYtKx4E+2W3kjv8zXp/7IddO4eW7GgGtxON08/WkRszKiOD87jvlZcXx5diphFhP3/TWft7eXS4CPMvLzlBBnITHSyr/uv5Abz+07GNNjbL22dbPbQnjpzrmkRoX6Nn4GT2s+LtzCf10zmYsnJvDAG7t46fPDA6pDa80vVxZQ1tDOty8ej1IKo0ExPzuOGelRLJ2ZyoaiGqpkstCooobzJkdubq7Oy8sbtvcTYiRr6XRiVOqkS+w6nG6+9eetrD1QzZrvX0RKVGifr6O15hfvFrBi/SG+Nj+Tn18z+YTRJ4VVLSx5fA0/vXoyd50/luLaNtJjQntd1+Vyc+VvP+OWeWO4fX7moH1OcfaUUlu11rnHH5cWuBB+Em4x9bk+utlk4KFlU9Aalq892Otc3uE6frNqn293oWc/O8SK9Ye4Y8HJwxs8M0qnpkbyRn4pP31zNxc+upq3tvfebm5XWSMHqlr45coCSoZg8wox+CTAhRih0qJtfHl2Gq9uLqaq2dP1sXpfFV99dhO//6SQX3+wj8KqZh5dtY9LJifys6tPHt7dls1IZXdZEy9vPILZaDhhyv/mQ57p/galePDN3TIEMQDITUwhRrBvLcriH1tL+Mkbu0mMtPLXLcXkJEUwMSmS5WsP8t7OCsLMRh65blq/k3aum53K6n1V3DQngw+/qGRDUS1aa9/zNh2sJSs+jFvmjeGhd75g5e6jXDkteTg+pjhD/bbAlVIrlFJVSqndxx3/T6XUPqXUHqXUr4euRCGCV2ZcGNfNSmPVF5W8vq2UxRMTeeXr83jkumnMzoiirKGd/142lfgIS7+vFRdu4S/fmMc1M1KYnxVLdXOnb9Nml1uTd7ieOWNjue28TDJibPxtS0m/rymtdP8aSAv8BeBJ4KXuA0qpi4BlwHStdadSKqGP5wohztLD103l3ouyGBMb1muy0IqvnUt+cQOLck5/vPj8LM+49PWFtWQnRFBQ0URzp5N542IwGhSXTE7k5c+P0NrpJMzSd0zc8+etmE1Gfn/TrNP/YOKs9dsC11qvBeqOO/wt4Fda607vNSeunymEGBTWECPj4sNPmOkZZTNz0cSEM1rvJD0mlNSoUDZ4F9ba5O3/PjfTs3jXkkmJOFxuPjtQ7XtOl8vNnS9s4fEP9wOedVtWfVHJqj1HfVvSieF1pjcxJwAXKKU2KaXWKKXO7etCpdTdSqk8pVRedXV1X5cJIYaRUooF2bFsPFiHy63ZfKiW9JhQ33DF3MxoIq0mPio41jb7w+pCPtlbxTNriqhrdfDOjnK09mwm3f0fgBheZxrgJiAamAd8H/i76qMZoLVerrXO1Vrnxsef/o96QoihMT8rjsb2Lh5+r4ANRbXMHRvrOxdiNHDRxAQ+2evZ43NXaSNPflLI3LExdDrdvLq5mLd3lJOTGIHZZGDtfk/jrLGti4rGdn99pKBzpgFeCryuPTYDbiBu8MoSQgy1+dmxmE0GVqw/hMVkZOmMlF7nF09KpK7VwSPvF/D1l7YQG25m+a25nJ8dx/K1B9lZ2shXctOYOzaGNfurcbk1Nz+7kUv/by0HvTdHxdA60wB/E7gYQCk1ATADNYNUkxBiGCREWNn8wGK++O/LyPvJEi6c0Psn5IUT4jEZFM+tO0SyPZQ/3ZaL3RbCHQsyaWzvQim4enoKCyfEU1jVwm8/PsCe8iYcTjffeCmPpg7ZXWio9TsKRSn1KrAIiFNKlQI/B1YAK7xDCx3A7VrGEwkRcKJs5j7P2UNDWH7bOVhDjJw3LtZ3s/SinASy4sNIifLs5blwQjz/814Bv/v4ALljovnupTnc8twmrv/jBs4ZE8352fFcNX1wx5O73Zp/bi3lsilJvRYKCzb9BrjW+qY+Tt0yyLUIIUaYiycmnnDMYFD88575GI2eQM9OCCfZuynzz66ZzPS0KB6/YQYr1h/mvZ0V/HNrKednxw0oaEvq2nhs1T4eWjrllP+5rNlfzQ9e20l1Syf3XpR95h8wwMlUeiHEaYsOM/tWWFRKcd/i8fzg8hymp0UBsGxmKm/du4AX75xDl0vzYUHlgF73n1tLeWt773XMT+bljUcAfMMgg5UEuBDirN04J4N/X3RiS3hmehSpUaG8v6tiQK+z1jvu/IUNhylrOPlolpK6NlbvqyLMbCTvcD2dzuAdgy4BLoQYMkoprpiaxGcHqmls76LT6eJIbavvvNutaWhzANDQ5mBHSQPXn5MGwBPeCUPH+/OmIxiU4kdXTKTT6Sa/uGHIP8dIJQEuhBhSV05PpsuleX1bKTf/aRMLH/2U21ds5uk1RVz8m0+Z88jHFFa1sK6wBreGm+akc9u8Mby2rZS9R5t6vVZHl4u/bynhkkmJLJ2ZikHB50W1fvpk/icBLoQYUrPSo0ixW3nonS/YVdrIbeeNYXdZI79auRe7zYzZaODX/9rL2v3VRFhNzEiL4j8uziYyNISH3v6i14JZ7+2soL6ti9vOG4M9NIQpKXYJcCGEGCpKKa4/J40ws5EVXzuX/142lXU/vJjV31vEm/8+n29eOI5VX1Ty3s4Kzs+Ow2Q0EGUz891Lc/j8YC0rdx/1vdbLG4+QFR/GeVmeWaPzs2LJL6mn3XGsH3zt/uqg2TpOAlwIMeTuXzKBLT9ZwvnjPRO2Q81GxsaFoZTirgvGkhBhodXhYmGPyUQ3z8lgUnIkD79XQEunk12ljWwvaeDWeWN8Y9LPy4qly6VZX+gZjfKPvBJuW7GZrzzzOdXNncP/QYeZBLgQYsgZDAqb+eTTTmxmEz+8fCKhIUYW5RxbmdpoUDy0dAoVje3cuPxzfv/JAWxmI1/y3uQEmDM2hsRIC//5aj6PfbCPB97YxYz0KKqaOrl9xeZRPxtUAlwI4XdfPieNHT+/lCS7tdfxOWNjePb2XA5Wt7Lqi0qunZXqG38OnvB/+z/OZ1qanSdXF5IRY+OlO+fwx1tms7+ymVuf3UR9q2O4P86wkV3phRAj3u6yRp74aD8/vXoyY2LDTjjvdLl5I7+MBdlxviVxP/yiknv/so2MGBsLsmL5qKCKL81O5buX5gx3+Wetr13pJcCFEKPWxoO1fOPFPBwuN9E2M51OF5sfXEKI0dP50O5w8exnB4mLsHDTnAw/V9u3vgJcNjUWQoxa88bFsvnBJWg06wtr+cZLeawrrOGinAQ+L6rlB6/toKSuHbPJwOJJCSREWPt/0RFE+sCFEKNaqNmIzWziwglxRFpNvLOjnOrmTu5+OQ+TwcCj10+ny+XmhfWH/V3qaZMWuBAiKFhMRi6fmsT7u47icLrp6HLx7O25ZMWHs3pfFS9vPMK3FmURYT31qom7ShtZubuC71wywdcV4y/SAhdCBI1rZqTQ0unk3Z0V3H3hOLLiwwG4Z2EWzR1OXt1cfMrnN3V08c2X83jq0yJ+tXLvcJR8ShLgQoigcd64WOLCzaRGhfZaR3x6WhTnZ8fx1KdFVJ5iFud/vbWHyuZOFk9M4Ll1h3h3Z3m/79npdPHrf+0dkolFMgpFCBFUdpY2YDMbyU6I6HW8qLqFq373GXPGxvLiHeeyq6yRzYfqcLo1zR1dFFa18MGeSu5bPJ57L8rmpj9tZFtxPWPjwpiYFEFqVChRNrMvqL+zZAJ2Wwh/WnuQh98v4KU755ywbd1AyTBCIYTox8sbj/DTN3czJSWSPeXHVkI0GhSpUaHMGxfDw9dNI8RooK7VwcufH2FPeSMHqloob2in0+kmwmKircvFZVMS+cWyqSx67FPOGRPNC3fMOeO6ZBihEEL045a5GazZV83mQ7V895IJ3DQ3A5vZiNlowHTcDcuYMDP3LRnv+1prTafTjTXEyNNrPH3kRVWttDlcPHjlpCGpVwJcCCG8lFI8c+s5uNwas+n0bhEqpbCGGAG4+4JxrDtQw7rCGm6Zl8H4xIh+nn1mJMCFEKIHo0FhNKizeg2DQfH4v81gxbrD3LNw3CBVdiIJcCGEGAIJEVZ+dMXEIX0PGUYohBABSgJcCCEClAS4EEIEKAlwIYQIUBLgQggRoCTAhRAiQEmACyFEgJIAF0KIADWsi1kppaqBI2f49DigZhDLGQpS4+CQGs/eSK8PpMbTMUZrfcJShsMa4GdDKZV3stW4RhKpcXBIjWdvpNcHUuNgkC4UIYQIUBLgQggRoAIpwJf7u4ABkBoHh9R49kZ6fSA1nrWA6QMXQgjRWyC1wIUQQvQgAS6EEAEqIAJcKXW5UmqfUqpQKfWjEVBPulJqtVKqQCm1Ryl1n/d4jFLqQ6XUAe/v0SOgVqNSKl8p9e5IrFEpFaWU+qdSaq/3z/O8EVjjd7x/z7uVUq8qpaz+rlEptUIpVaWU2t3jWJ81KaV+7P3+2aeUusyPNT7q/bveqZR6QykVNdJq7HHue0oprZSK82eNpzLiA1wpZQT+AFwBTAZuUkpN9m9VOIHvaq0nAfOAe701/Qj4WGs9HvjY+7W/3QcU9Ph6pNX4W+BfWuuJwAw8tY6YGpVSqcC3gVyt9VTACNw4Amp8Abj8uGMnrcn7b/NGYIr3OU95v6/8UeOHwFSt9XRgP/DjEVgjSql04BKguMcxf9XYpxEf4MAcoFBrfVBr7QD+CizzZ0Fa6wqt9Tbv42Y8oZPqretF72UvAtf6pUAvpVQacBXwbI/DI6ZGpVQkcCHwHIDW2qG1bmAE1ehlAkKVUibABpTj5xq11muBuuMO91XTMuCvWutOrfUhoBDP99Ww16i1XqW1dnq/3AikjbQavf4P+AHQc5SHX2o8lUAI8FSgpMfXpd5jI4JSKhOYBWwCErXWFeAJeSDBj6UBPIHnH6G7x7GRVOM4oBp43tvN86xSKmwk1ai1LgMew9MSqwAatdarRlKNPfRV00j9HroTWOl9PGJqVEotBcq01juOOzViauwWCAF+su2hR8TYR6VUOPAacL/Wusnf9fSklLoaqNJab/V3LadgAmYDf9RazwJa8X+XTi/efuRlwFggBQhTSt3i36pO24j7HlJKPYinK/KV7kMnuWzYa1RK2YAHgZ+d7PRJjvn1zzEQArwUSO/xdRqeH2H9SikVgie8X9Fav+49XKmUSvaeTwaq/FUfsABYqpQ6jKfb6WKl1J8ZWTWWAqVa603er/+JJ9BHUo1LgENa62qtdRfwOjB/hNXYra+aRtT3kFLqduBq4Kv62ESUkVJjFp7/rHd4v3fSgG1KqSRGTo0+gRDgW4DxSqmxSikznpsIb/uzIKWUwtNvW6C1frzHqbeB272PbwfeGu7aummtf6y1TtNaZ+L5M/tEa30LI6vGo0CJUirHe2gx8AUjqEY8XSfzlFI279/7Yjz3PEZSjd36qult4EallEUpNRYYD2z2Q30opS4Hfggs1Vq39Tg1ImrUWu/SWidorTO93zulwGzvv9URUWMvWusR/wu4Es8d6yLgwRFQz/l4fnTaCWz3/roSiMVz9/+A9/cYf9fqrXcR8K738YiqEZgJ5Hn/LN8EokdgjQ8Be4HdwMuAxd81Aq/i6ZPvwhMyd52qJjzdAkXAPuAKP9ZYiKcfufv75umRVuNx5w8Dcf6s8VS/ZCq9EEIEqEDoQhFCCHESEuBCCBGgJMCFECJASYALIUSAkgAXQogAJQEuhBABSgJcCCEC1P8HeZ65Td3LCuAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf33bfa",
   "metadata": {},
   "source": [
    "## Examining the submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30f63d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v110</th>\n",
       "      <th>v111</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v114</th>\n",
       "      <th>v115</th>\n",
       "      <th>v116</th>\n",
       "      <th>v117</th>\n",
       "      <th>v118</th>\n",
       "      <th>v119</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_austin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_austin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_austin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3_austin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_austin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  v0  v1  v2  v3  v4  v5  v6  v7  v8  ...  v110  v111  v112  v113  \\\n",
       "0  0_austin   0   0   0   0   0   0   0   0   0  ...     0     0     0     0   \n",
       "1  1_austin   0   0   0   0   0   0   0   0   0  ...     0     0     0     0   \n",
       "2  2_austin   0   0   0   0   0   0   0   0   0  ...     0     0     0     0   \n",
       "3  3_austin   0   0   0   0   0   0   0   0   0  ...     0     0     0     0   \n",
       "4  4_austin   0   0   0   0   0   0   0   0   0  ...     0     0     0     0   \n",
       "\n",
       "   v114  v115  v116  v117  v118  v119  \n",
       "0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub = pd.read_csv('sample_submission.csv')\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3fb6c5f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def city_data(city_name):\n",
    "    if city_name == \"austin\":\n",
    "        return {'austin':6325}\n",
    "    elif city_name == \"miami\":\n",
    "        return {'miami':7971}\n",
    "    elif city_name == \"pittsburgh\":\n",
    "        return {'pittsburgh': 6361}\n",
    "    elif city_name == \"dearborn\":\n",
    "        return {'dearborn':3671}\n",
    "    elif city_name == \"washington-dc\":\n",
    "        return {'washington-dc':3829}\n",
    "    else:\n",
    "        return {'palo-alto':1686}\n",
    "\n",
    "num_pred_steps = 60\n",
    "all_preds = np.zeros(shape=(0, num_pred_steps * 2))\n",
    "city_col = np.array([])\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "\n",
    "test_data = [6325, 7971, 6361, 3671, 3829, 1686]\n",
    "for city_name in cities:\n",
    "\n",
    "    inpu, out = get_city_trajectories(city=city_name, split=\"test\")\n",
    "    all_end_pos_x = []\n",
    "    all_end_pos_y = []\n",
    "    for i in inpu:\n",
    "        end_pos_x = i[49][0]\n",
    "        end_pos_y = i[49][1]\n",
    "        all_end_pos_x.append(end_pos_x)\n",
    "        all_end_pos_y.append(end_pos_y)\n",
    "    city_test = city_data(city_name)\n",
    "    test_x = get_input_tensor(city_test, \"test\")\n",
    "    test_loader = DataLoader(SequenceDataset(test_x.to(device), torch.tensor(np.zeros((test_x.shape[0],120)))))\n",
    "    \n",
    "    outcome = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample_batch in test_loader:\n",
    "            x_test = sample_batch[0]\n",
    "            \n",
    "            output = model(x_test.float()).cpu().detach().numpy()\n",
    "            outcome.append(output)\n",
    "        \n",
    "    #outcome = model(test_loader)\n",
    "    \n",
    "    test_pred_arr = np.array(outcome)\n",
    "    test_pred_arr_reshaped = np.reshape(test_pred_arr, newshape=(test_x.shape[0], num_pred_steps * 2))\n",
    "    for i in range(test_pred_arr_reshaped.shape[0]):\n",
    "        for j in range(60):\n",
    "        \n",
    "            test_pred_arr_reshaped[i][j*2] = test_pred_arr_reshaped[i][j*2] + all_end_pos_x[i]\n",
    "            test_pred_arr_reshaped[i][j*2+1] = test_pred_arr_reshaped[i][j*2+1] + all_end_pos_y[i]\n",
    "    \n",
    "    #test_pred_arr_reshaped = np.reshape(test_pred_arr, newshape=(test_x.shape[0], num_pred_steps * 2))\n",
    "    \n",
    "    all_preds = np.r_[all_preds, test_pred_arr_reshaped]\n",
    "    \n",
    "    city_col = np.r_[city_col, [str(i) + \"_\" + city_name for i in range(test_pred_arr.shape[0])]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a5e33ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(np.c_[city_col, all_preds], columns=[np.r_[[\"ID\"], [\"v\" + str(i) for i in range(120)]]])\n",
    "sub_df.to_csv('test_sub_2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9fc42bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v110</th>\n",
       "      <th>v111</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v114</th>\n",
       "      <th>v115</th>\n",
       "      <th>v116</th>\n",
       "      <th>v117</th>\n",
       "      <th>v118</th>\n",
       "      <th>v119</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_austin</td>\n",
       "      <td>-13.103030</td>\n",
       "      <td>-566.904053</td>\n",
       "      <td>-14.465156</td>\n",
       "      <td>-566.223511</td>\n",
       "      <td>-16.038363</td>\n",
       "      <td>-565.575745</td>\n",
       "      <td>-17.578993</td>\n",
       "      <td>-565.082947</td>\n",
       "      <td>-19.039581</td>\n",
       "      <td>...</td>\n",
       "      <td>-99.631897</td>\n",
       "      <td>-536.512512</td>\n",
       "      <td>-101.038956</td>\n",
       "      <td>-536.040344</td>\n",
       "      <td>-102.273689</td>\n",
       "      <td>-535.657349</td>\n",
       "      <td>-103.311226</td>\n",
       "      <td>-535.302612</td>\n",
       "      <td>-104.152504</td>\n",
       "      <td>-535.019165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_austin</td>\n",
       "      <td>-344.724487</td>\n",
       "      <td>11.986337</td>\n",
       "      <td>-344.227417</td>\n",
       "      <td>11.659631</td>\n",
       "      <td>-343.600952</td>\n",
       "      <td>11.330400</td>\n",
       "      <td>-342.993744</td>\n",
       "      <td>11.096383</td>\n",
       "      <td>-342.416870</td>\n",
       "      <td>...</td>\n",
       "      <td>-302.780487</td>\n",
       "      <td>-7.724325</td>\n",
       "      <td>-302.132538</td>\n",
       "      <td>-8.195616</td>\n",
       "      <td>-301.574524</td>\n",
       "      <td>-8.621778</td>\n",
       "      <td>-301.097076</td>\n",
       "      <td>-8.957762</td>\n",
       "      <td>-300.714539</td>\n",
       "      <td>-9.248537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_austin</td>\n",
       "      <td>50.982666</td>\n",
       "      <td>-247.707520</td>\n",
       "      <td>50.974007</td>\n",
       "      <td>-247.750214</td>\n",
       "      <td>50.965851</td>\n",
       "      <td>-247.802612</td>\n",
       "      <td>50.953869</td>\n",
       "      <td>-247.850647</td>\n",
       "      <td>50.951221</td>\n",
       "      <td>...</td>\n",
       "      <td>49.641914</td>\n",
       "      <td>-248.418304</td>\n",
       "      <td>49.597363</td>\n",
       "      <td>-248.392044</td>\n",
       "      <td>49.559330</td>\n",
       "      <td>-248.367859</td>\n",
       "      <td>49.528313</td>\n",
       "      <td>-248.344940</td>\n",
       "      <td>49.503227</td>\n",
       "      <td>-248.328949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3_austin</td>\n",
       "      <td>-113.809677</td>\n",
       "      <td>1798.274536</td>\n",
       "      <td>-113.814751</td>\n",
       "      <td>1798.389526</td>\n",
       "      <td>-113.819565</td>\n",
       "      <td>1798.508789</td>\n",
       "      <td>-113.826157</td>\n",
       "      <td>1798.614502</td>\n",
       "      <td>-113.827621</td>\n",
       "      <td>...</td>\n",
       "      <td>-116.250610</td>\n",
       "      <td>1802.670410</td>\n",
       "      <td>-116.313576</td>\n",
       "      <td>1802.730225</td>\n",
       "      <td>-116.372261</td>\n",
       "      <td>1802.775391</td>\n",
       "      <td>-116.416260</td>\n",
       "      <td>1802.816284</td>\n",
       "      <td>-116.454857</td>\n",
       "      <td>1802.849365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_austin</td>\n",
       "      <td>1195.851440</td>\n",
       "      <td>-638.044189</td>\n",
       "      <td>1196.960083</td>\n",
       "      <td>-638.599121</td>\n",
       "      <td>1198.214966</td>\n",
       "      <td>-639.148926</td>\n",
       "      <td>1199.436646</td>\n",
       "      <td>-639.576477</td>\n",
       "      <td>1200.591309</td>\n",
       "      <td>...</td>\n",
       "      <td>1260.153809</td>\n",
       "      <td>-667.780029</td>\n",
       "      <td>1261.119263</td>\n",
       "      <td>-668.238159</td>\n",
       "      <td>1261.958252</td>\n",
       "      <td>-668.641785</td>\n",
       "      <td>1262.666748</td>\n",
       "      <td>-668.979187</td>\n",
       "      <td>1263.241089</td>\n",
       "      <td>-669.251831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID           v0           v1           v2           v3           v4  \\\n",
       "0  0_austin   -13.103030  -566.904053   -14.465156  -566.223511   -16.038363   \n",
       "1  1_austin  -344.724487    11.986337  -344.227417    11.659631  -343.600952   \n",
       "2  2_austin    50.982666  -247.707520    50.974007  -247.750214    50.965851   \n",
       "3  3_austin  -113.809677  1798.274536  -113.814751  1798.389526  -113.819565   \n",
       "4  4_austin  1195.851440  -638.044189  1196.960083  -638.599121  1198.214966   \n",
       "\n",
       "            v5           v6           v7           v8  ...         v110  \\\n",
       "0  -565.575745   -17.578993  -565.082947   -19.039581  ...   -99.631897   \n",
       "1    11.330400  -342.993744    11.096383  -342.416870  ...  -302.780487   \n",
       "2  -247.802612    50.953869  -247.850647    50.951221  ...    49.641914   \n",
       "3  1798.508789  -113.826157  1798.614502  -113.827621  ...  -116.250610   \n",
       "4  -639.148926  1199.436646  -639.576477  1200.591309  ...  1260.153809   \n",
       "\n",
       "          v111         v112         v113         v114         v115  \\\n",
       "0  -536.512512  -101.038956  -536.040344  -102.273689  -535.657349   \n",
       "1    -7.724325  -302.132538    -8.195616  -301.574524    -8.621778   \n",
       "2  -248.418304    49.597363  -248.392044    49.559330  -248.367859   \n",
       "3  1802.670410  -116.313576  1802.730225  -116.372261  1802.775391   \n",
       "4  -667.780029  1261.119263  -668.238159  1261.958252  -668.641785   \n",
       "\n",
       "          v116         v117         v118         v119  \n",
       "0  -103.311226  -535.302612  -104.152504  -535.019165  \n",
       "1  -301.097076    -8.957762  -300.714539    -9.248537  \n",
       "2    49.528313  -248.344940    49.503227  -248.328949  \n",
       "3  -116.416260  1802.816284  -116.454857  1802.849365  \n",
       "4  1262.666748  -668.979187  1263.241089  -669.251831  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub = pd.read_csv('test_sub_2.csv')\n",
    "test_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fcc9d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "be4522dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    1024 MB |    5819 MB |  188664 GB |  188663 GB |\\n|       from large pool |    1016 MB |    5813 MB |  175662 GB |  175661 GB |\\n|       from small pool |       8 MB |      73 MB |   13002 GB |   13002 GB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    1024 MB |    5819 MB |  188664 GB |  188663 GB |\\n|       from large pool |    1016 MB |    5813 MB |  175662 GB |  175661 GB |\\n|       from small pool |       8 MB |      73 MB |   13002 GB |   13002 GB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    1240 MB |    6134 MB |    6208 MB |    4968 MB |\\n|       from large pool |    1226 MB |    6058 MB |    6122 MB |    4896 MB |\\n|       from small pool |      14 MB |      76 MB |      86 MB |      72 MB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  220253 KB |    1108 MB |  187371 GB |  187371 GB |\\n|       from large pool |  215029 KB |    1105 MB |  174103 GB |  174103 GB |\\n|       from small pool |    5224 KB |       7 MB |   13267 GB |   13267 GB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     108    |    1627    |   76019 K  |   76019 K  |\\n|       from large pool |      38    |      67    |   25904 K  |   25904 K  |\\n|       from small pool |      70    |    1598    |   50115 K  |   50115 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     108    |    1627    |   76019 K  |   76019 K  |\\n|       from large pool |      38    |      67    |   25904 K  |   25904 K  |\\n|       from small pool |      70    |    1598    |   50115 K  |   50115 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      21    |      62    |      68    |      47    |\\n|       from large pool |      14    |      24    |      25    |      11    |\\n|       from small pool |       7    |      38    |      43    |      36    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      39    |     105    |   35214 K  |   35214 K  |\\n|       from large pool |      15    |      24    |   14647 K  |   14647 K  |\\n|       from small pool |      24    |      88    |   20566 K  |   20566 K  |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674eda96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9401c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
